{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6046133",
   "metadata": {},
   "source": [
    "# BME3053C - Computer Applications for BME\n",
    "\n",
    "<br/>\n",
    "\n",
    "<h1 align=\"center\">Convolutional Neural Networks</h1>\n",
    "\n",
    "---\n",
    "\n",
    "<center><h2>Lesson: 11</h2></center>\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/uf-bme/bme3053c/blob/main/lessons/11_Convolutional_Neural_Networks.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4734d37c",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Convolutional Neural Networks (CNNs) are a specialized type of deep learning model that excels in processing data with grid-like structures, such as images. Originally developed for image recognition, CNNs are now widely used in applications like video analysis, natural language processing, and biomedical imaging. This introduction will guide you through the core concepts of CNNs, including their architecture, the convolution operation, pooling layers, and the training process, with in-depth explanations and practical examples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f542544",
   "metadata": {},
   "source": [
    "## When to Use CNNs vs Other Neural Networks\n",
    "\n",
    "CNNs are particularly powerful when working with image data or spatial data where local features are essential. Compared to fully connected neural networks, which treat all inputs as equally important, CNNs leverage spatial hierarchies in data, making them more efficient and effective for tasks like image classification, object detection, and medical imaging.\n",
    "\n",
    "For example, a CNN is ideal for recognizing handwritten digits in the MNIST dataset, where spatial features such as curves and edges are critical. On the other hand, a fully connected neural network might be more suitable for predicting housing prices based on numerical input features like square footage or number of bedrooms, as these inputs do not have a spatial relationship. Fully connected neural networks work well for simple tasks with non-image data, while CNNs shine in scenarios that require detecting patterns across different spatial locations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb25a490",
   "metadata": {},
   "source": [
    "## When to Use CNNs in Biomedical Applications\n",
    "\n",
    "CNNs are ideal for tasks such as medical image analysis, where detecting tumors, analyzing radiographic images, or identifying anatomical structures requires recognizing spatial features across images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305a2b0f",
   "metadata": {},
   "source": [
    "## Pros and Cons of Convolutional Neural Networks\n",
    "\n",
    "### Pros\n",
    "\n",
    "- **Automatic Feature Extraction**: CNNs can learn relevant features directly from images without requiring manual feature engineering.\n",
    "- **Translation Invariance**: They are robust to changes in position within images, making them ideal for classification tasks where objects may appear in various locations.\n",
    "- **Efficient Parameter Sharing**: Convolutional layers use shared weights, reducing the number of parameters compared to fully connected layers.\n",
    "\n",
    "### Cons\n",
    "\n",
    "- **High Computational Cost**: CNNs often require significant computational resources and memory, especially for large models.\n",
    "- **Data Requirements**: They perform best with large datasets. Training a CNN on a small dataset might lead to overfitting without proper regularization techniques, such as dropout or L2 regularization. Alternatively, transfer learning can be employed, where a pre-trained model is fine-tuned on a smaller dataset to achieve better performance.\n",
    "- **Interpretability**: Understanding what specific features are being learned can be challenging, making them less interpretable compared to simpler models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82059fea",
   "metadata": {},
   "source": [
    "## Convolution\n",
    "\n",
    "Convolution is a fundamental operation in CNNs, especially when dealing with image data. In this context, convolution involves sliding a small filter (kernel) over the input image to produce a feature map that highlights specific patterns, such as edges or textures.\n",
    "\n",
    "Imagine the filter as a small window that moves across the image, focusing on a few pixels at a time. Each time the filter moves, it calculates a weighted sum of the pixel values under it. This process allows the CNN to learn important features, like edges and corners, which are then used for further analysis in deeper layers.\n",
    "\n",
    "<center><img  src=\"https://github.com/uf-bme/bme3053c/raw/main/files/conv-slide.gif\"/></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8742412d",
   "metadata": {},
   "source": [
    "## Filter Size, Padding, and Stride\n",
    "\n",
    "### Filter Size\n",
    "The filter (or kernel) size determines how many pixels the convolution operation considers at once. Common filter sizes are 3x3, 5x5, and 7x7. Larger filters can capture more context but require more computation and parameters:\n",
    "- Small filters (3x3): Good for detecting fine details and edges\n",
    "- Large filters (5x5, 7x7): Better for capturing broader patterns\n",
    "\n",
    "### Padding\n",
    "Padding adds extra pixels around the input image before convolution. This helps:\n",
    "- Preserve the spatial dimensions of the input\n",
    "- Avoid information loss at the edges\n",
    "- Control the size of the output feature maps\n",
    "\n",
    "\n",
    "### Stride\n",
    "Stride controls how many pixels the filter moves at each step:\n",
    "- Stride 1: Filter moves one pixel at a time\n",
    "- Stride 2: Filter moves two pixels at a time\n",
    "\n",
    "Larger strides reduce the spatial dimensions of the output feature map, similar to pooling layers, but may lose information in the process.\n",
    "\n",
    "[Examples of padding and strides](https://hannibunny.github.io/mlbook/neuralnetworks/convolutionDemos.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3b2696",
   "metadata": {},
   "source": [
    "## Pooling Layers\n",
    "\n",
    "Pooling is used to reduce the dimensions of feature maps, decreasing computational complexity and preventing overfitting. The most common type is Max Pooling, which takes the maximum value from a patch of the feature map. Pooling layers make the representation more manageable while retaining important features.\n",
    "\n",
    "- **Max Pooling**: Takes the highest value in each patch.\n",
    "- **Average Pooling**: Takes the average value.\n",
    "\n",
    "- Pooling helps achieve translational invariance by taking the maximum (or average) value within each patch. \n",
    "  - For example, if a feature like an edge moves slightly within a pooling window, the max pooling operation \n",
    "will still detect it and output the same value, making the network less sensitive to small spatial shifts \n",
    "in the input image.\n",
    "\n",
    "<br/>\n",
    "\n",
    "<center><img  src=\"https://github.com/uf-bme/bme3053c/raw/main/files/maxpool.gif\"/></center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a13b3982",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    from torch.utils.data import DataLoader, random_split\n",
    "    from torchvision import transforms\n",
    "    import torch.utils.data as data\n",
    "    from tqdm import tqdm\n",
    "except ImportError:\n",
    "    %pip install torch torchvision torchaudio \n",
    "    %pip install tqdm\n",
    "    import torch.utils.data as data\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim \n",
    "    from torch.utils.data import DataLoader, random_split\n",
    "    from torchvision import transforms\n",
    "    from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac4f4e2",
   "metadata": {},
   "source": [
    "https://github.com/tensorflow/lucid?tab=readme-ov-file#recomended-reading\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c77287",
   "metadata": {},
   "source": [
    "## MedMNIST\n",
    "MedMNIST (https://medmnist.com/) is a collection of 12 pre-processed medical image datasets that follow the same format as the classic MNIST dataset. Each dataset represents a specific medical imaging task, such as organ classification, disease detection, or tissue identification. The datasets are standardized to be lightweight, making them ideal for rapid prototyping and educational purposes.\n",
    "\n",
    "Key features of MedMNIST:\n",
    "- All images are pre-processed to 28x28 pixels\n",
    "- Includes both 2D and 3D medical imaging data\n",
    "- Covers various medical domains (pathology, radiology, etc.)\n",
    "- Suitable for both binary and multi-class classification tasks\n",
    "- Comes with standardized evaluation metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db69b984",
   "metadata": {},
   "source": [
    "\n",
    "## PathMNIST Dataset\n",
    "\n",
    "The PathMNIST dataset is a part of the MedMNIST collection, which is a set of standardized biomedical image datasets for machine learning. \n",
    "\n",
    "PathMNIST contains images of histopathologic scans of colorectal cancer. The dataset is divided into three splits: training, validation, and testing. Each image is a 3-channel RGB image with a size of 28x28 pixels. The dataset includes 9 different classes:\n",
    "\n",
    "0. **adipose**  \n",
    "   - Refers to fat tissue, often found surrounding organs and structures in the body.\n",
    "\n",
    "1. **background**  \n",
    "   - The background of the image, which does not contain any relevant tissue or structures.\n",
    "\n",
    "2. **debris**  \n",
    "   - Refers to cellular or tissue remnants that may appear as non-specific or irrelevant material in the sample.\n",
    "\n",
    "3. **lymphocytes**  \n",
    "   - A type of white blood cell involved in immune responses. In histopathology, their presence can be indicative of inflammation or immune response.\n",
    "\n",
    "4. **mucus**  \n",
    "   - The gel-like substance that coats various tissues, such as the digestive tract, serving as a protective layer.\n",
    "\n",
    "5. **smooth muscle**  \n",
    "   - Muscle tissue that is not under voluntary control, found in organs like the intestines, where it helps in movement and digestion.\n",
    "\n",
    "6. **normal colon mucosa**  \n",
    "   - The normal tissue lining of the colon, responsible for absorbing nutrients and protecting the colon wall.\n",
    "\n",
    "7. **cancer-associated stroma**  \n",
    "   - The connective tissue surrounding cancer cells. In cancer, the stroma may undergo changes that support tumor growth and spread.\n",
    "\n",
    "8. **colorectal adenocarcinoma epithelium**  \n",
    "   - Cancerous epithelial tissue found in colorectal adenocarcinoma. This is a type of cancer originating in the glands of the colon or rectum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e137cd0d",
   "metadata": {},
   "source": [
    "## Loading the Dataset\n",
    "\n",
    "To demonstrate the capabilities of CNNs, we will work with the PathMNIST dataset, which contains histopathologic images of colorectal cancer tissue. This dataset is part of the MedMNIST collection and includes 9 different tissue classes. We will apply two key transformations to prepare the data:\n",
    "\n",
    "1. ToTensor() - Converts the PIL Image to a PyTorch tensor and scales pixel values from [0, 255] to [0, 1]\n",
    "2. Normalize() - Normalizes the tensor with mean 0.5 and standard deviation 0.5, resulting in values between -1 and 1\n",
    "\n",
    "These transformations help standardize the input data and improve model training. The images are already in the correct format of 3x28x28 RGB images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "74b6a396",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import medmnist\n",
    "    from medmnist import INFO, Evaluator\n",
    "\n",
    "except ImportError:\n",
    "    %pip install medmnist\n",
    "    import medmnist\n",
    "    from medmnist import INFO, Evaluator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6e192ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_flag = 'pathmnist'\n",
    "# data_flag = 'breastmnist'\n",
    "download = True\n",
    "\n",
    "NUM_EPOCHS = 3\n",
    "BATCH_SIZE = 128\n",
    "lr = 0.001\n",
    "\n",
    "info = INFO[data_flag]\n",
    "task = info['task']\n",
    "n_channels = info['n_channels']\n",
    "n_classes = len(info['label'])\n",
    "\n",
    "DataClass = getattr(medmnist, info['python_class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90973b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[.5], std=[.5])\n",
    "])\n",
    "download = True\n",
    "# load the data\n",
    "\n",
    "train_dataset = DataClass(split='train', transform=data_transform, download=download)\n",
    "test_dataset = DataClass(split='test', transform=data_transform, download=download)\n",
    "\n",
    "pil_dataset = DataClass(split='train', download=download)\n",
    "\n",
    "# encapsulate data into dataloader form\n",
    "train_loader = data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "train_loader_at_eval = data.DataLoader(dataset=train_dataset, batch_size=2*BATCH_SIZE, shuffle=False)\n",
    "test_loader = data.DataLoader(dataset=test_dataset, batch_size=2*BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eed3334",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.montage(length=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07386f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 3 examples from each class\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "classes =train_dataset.info['label']\n",
    "for class_idx in range(9):\n",
    "    # Get indices for this class\n",
    "    indices = np.where(train_dataset.labels == class_idx)[0][:3]\n",
    "    \n",
    "    for i, idx in enumerate(indices):\n",
    "        plt.subplot(9, 3, class_idx * 3 + i + 1)\n",
    "        plt.imshow(train_dataset.imgs[idx].squeeze(), cmap='gray')\n",
    "        plt.title(f'{classes[str(class_idx)]}')\n",
    "        plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39eff661",
   "metadata": {},
   "source": [
    "Creating a Custom CNN Model\n",
    "\n",
    "We'll create a custom Convolutional Neural Network (CNN) model for classifying the PathMNIST dataset.\n",
    "\n",
    "Input:\n",
    "- RGB images of size 28x28x3 pixels\n",
    "- Images are normalized to [-1, 1] range using transforms.Normalize((0.5,), (0.5,))\n",
    "- Batch size of 64 images\n",
    "\n",
    "Our CNN architecture consists of:\n",
    "\n",
    "1. Input Layer: Takes RGB images (3 channels)\n",
    "2. Multiple Convolutional Blocks:\n",
    "   - Conv2D layers with increasing filters (16->64)\n",
    "   - Batch Normalization for training stability \n",
    "   - ReLU activation\n",
    "   - MaxPooling to reduce spatial dimensions\n",
    "3. Fully Connected Layers:\n",
    "   - Flattens convolutional features\n",
    "   - Three dense layers (128->128->9 classes)\n",
    "   - ReLU activation between dense layers\n",
    "\n",
    "The model uses standard practices like batch normalization and max pooling to improve training and reduce overfitting. The architecture gradually increases the number of filters while reducing spatial dimensions, allowing it to learn hierarchical features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "672f94b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a simple CNN model\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 16, kernel_size=3),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 16, kernel_size=3),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(16, 64, kernel_size=3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU())\n",
    "        \n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64 * 4 * 4, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "model = Net(in_channels=n_channels, num_classes=n_classes)\n",
    "    \n",
    "# define loss function and optimizer\n",
    "if task == \"multi-label, binary-class\":\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "else:\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24128158",
   "metadata": {},
   "source": [
    "## Loss Functions\n",
    "\n",
    "Loss functions measure how well a model is performing by calculating the difference between predicted outputs and actual target values. They provide a single number that the model tries to minimize during training.\n",
    "\n",
    "Common loss functions in PyTorch include:\n",
    "\n",
    "BCEWithLogitsLoss (Binary Cross Entropy with Logits Loss):\n",
    "- Measures error for binary classification (0 or 1 outputs)\n",
    "- Handles multiple independent binary predictions per sample\n",
    "- Internally applies sigmoid to convert raw model outputs to probabilities\n",
    "- More stable than separate sigmoid + BCE\n",
    "- Examples: Detecting multiple objects in an image, multi-disease diagnosis\n",
    "\n",
    "CrossEntropyLoss:\n",
    "- Measures error for multi-class classification (picking 1 class from many)\n",
    "- Internally applies softmax to get class probabilities that sum to 1\n",
    "- Good for problems where each input belongs to exactly one class\n",
    "- Examples: Classifying images into categories, digit recognition\n",
    "\n",
    "The choice of loss function depends on your task:\n",
    "- Use BCEWithLogitsLoss when samples can belong to multiple classes\n",
    "- Use CrossEntropyLoss when samples belong to exactly one class\n",
    "\n",
    "Our code uses BCEWithLogitsLoss for multi-label tasks and CrossEntropyLoss for standard classification.\n",
    "\n",
    "For more details: https://pytorch.org/docs/stable/nn.html#loss-functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9e397e",
   "metadata": {},
   "source": [
    "## Training the CNN\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9ce9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    \n",
    "    model.train()\n",
    "    for inputs, targets in tqdm(train_loader):\n",
    "        # forward + backward + optimize\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        if task == 'multi-label, binary-class':\n",
    "            targets = targets.to(torch.float32)\n",
    "            loss = criterion(outputs, targets)\n",
    "        else:\n",
    "            targets = targets.squeeze().long()\n",
    "            loss = criterion(outputs, targets)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220ebb57",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df541b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation\n",
    "\n",
    "def test(split):\n",
    "    model.eval()\n",
    "    y_true = torch.tensor([])\n",
    "    y_score = torch.tensor([])\n",
    "    \n",
    "    data_loader = train_loader_at_eval if split == 'train' else test_loader\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in data_loader:\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            if task == 'multi-label, binary-class':\n",
    "                targets = targets.to(torch.float32)\n",
    "                outputs = outputs.softmax(dim=-1)\n",
    "            else:\n",
    "                targets = targets.squeeze().long()\n",
    "                outputs = outputs.softmax(dim=-1)\n",
    "                targets = targets.float().resize_(len(targets), 1)\n",
    "\n",
    "            y_true = torch.cat((y_true, targets), 0)\n",
    "            y_score = torch.cat((y_score, outputs), 0)\n",
    "\n",
    "        y_true = y_true.numpy()\n",
    "        y_score = y_score.detach().numpy()\n",
    "        \n",
    "        evaluator = Evaluator(data_flag, split)\n",
    "        metrics = evaluator.evaluate(y_score)\n",
    "    \n",
    "        print('%s  auc: %.3f  acc:%.3f' % (split, *metrics))\n",
    "\n",
    "        \n",
    "print('==> Evaluating ...')\n",
    "test('train')\n",
    "test('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55e311c",
   "metadata": {},
   "source": [
    "Area Under the ROC Curve (AUC)\n",
    "\n",
    "AUC measures the model's ability to distinguish between classes by plotting the True Positive Rate (TPR)\n",
    "against the False Positive Rate (FPR) at various classification thresholds.\n",
    "\n",
    "TPR = TP / (TP + FN)  True Positive Rate (Sensitivity)\n",
    "FPR = FP / (FP + TN)  False Positive Rate (1 - Specificity)\n",
    "\n",
    "Where:\n",
    "TP = True Positives\n",
    "TN = True Negatives\n",
    "FP = False Positives\n",
    "FN = False Negatives\n",
    "\n",
    "AUC = ∫ TPR d(FPR)  Area under the ROC curve\n",
    "\n",
    "An AUC of:\n",
    "- 1.0: Perfect classification\n",
    "- 0.5: Random chance\n",
    "- <0.5: Worse than random\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c844ca",
   "metadata": {},
   "source": [
    "## Visualizing Feature Maps\n",
    "\n",
    "One of the ways to understand how CNNs work is to visualize feature maps, which show the output of convolutional layers for a given input image. This helps provide insight into the kind of features being learned at different stages of the network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e8b126",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "sample_image_id=3\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Get a sample image and its label from test loader\n",
    "sample_images, sample_labels = next(iter(test_loader))\n",
    "sample_image = sample_images[sample_image_id].unsqueeze(0).to(device)\n",
    "sample_label = sample_labels[sample_image_id]\n",
    "\n",
    "# Create a hook to capture feature maps\n",
    "activation = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "# Register hooks for all conv layers\n",
    "hooks = []\n",
    "conv_layers = []\n",
    "for module in model.modules():\n",
    "    if isinstance(module, torch.nn.Conv2d):\n",
    "        conv_layers.append(module)\n",
    "        if len(conv_layers) <= 5:  # Get first 5 conv layers\n",
    "            hooks.append(module.register_forward_hook(get_activation(f'layer{len(conv_layers)}')))\n",
    "\n",
    "# Move model to same device as input\n",
    "model = model.to(device)\n",
    "\n",
    "# Forward pass\n",
    "_ = model(sample_image)\n",
    "\n",
    "# Plot the input image and feature maps\n",
    "plt.figure(figsize=(15, 15))\n",
    "\n",
    "# Plot input image\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.imshow(sample_images[sample_image_id].permute(1, 2, 0))\n",
    "plt.title(classes[str(sample_label.item())])\n",
    "plt.axis('off')\n",
    "\n",
    "# Plot feature maps for each conv layer\n",
    "for layer_idx in range(5):\n",
    "    if f'layer{layer_idx+1}' in activation:\n",
    "        feature_maps = activation[f'layer{layer_idx+1}']\n",
    "        plt.subplot(2, 3, layer_idx+2)\n",
    "        # Show first feature map from each conv layer\n",
    "        plt.imshow(feature_maps[0, 0].cpu().numpy(), cmap='gray')\n",
    "        plt.title(f'Conv{layer_idx+1} Feature Map')\n",
    "        plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Remove the hooks\n",
    "for hook in hooks:\n",
    "    hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e03873d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model predictions on test set\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "all_images = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        # Convert labels to long type for criterion\n",
    "        labels = labels.squeeze().long()\n",
    "        \n",
    "        # Store predictions, labels and images\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_images.extend(images.cpu())\n",
    "\n",
    "# Find indices where predictions were wrong\n",
    "mistake_indices = [i for i in range(len(all_preds)) if all_preds[i] != all_labels[i]]\n",
    "\n",
    "# Plot up to 6 examples where network made mistakes\n",
    "num_plots = min(6, len(mistake_indices))\n",
    "plt.figure(figsize=(15, 15))\n",
    "for i in range(num_plots):\n",
    "    idx = mistake_indices[i]\n",
    "    # Get the corresponding image and true label\n",
    "    image = all_images[idx]\n",
    "    label = all_labels[idx]\n",
    "    pred = all_preds[idx]\n",
    "    \n",
    "    plt.subplot(2, 3, i+1)\n",
    "    plt.imshow(image.permute(1, 2, 0))  # Properly permute channels for display\n",
    "    plt.title(f'True: {classes[str(label)]}\\nIncorrect Pred: {classes[str(pred)]}')\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622000a3",
   "metadata": {},
   "source": [
    "### ✏️**Exercise**\n",
    "\n",
    "In this exercise, you will train a simple CNN model (Net) on the ChestMNIST dataset. ChestMNIST is a large dataset of chest X-ray images that can be used to detect various chest conditions.\n",
    "\n",
    "The dataset contains chest X-ray images classified into two classes:\n",
    "- Class 0: Normal - X-rays showing healthy chest anatomy with no abnormalities\n",
    "- Class 1: Abnormal - X-rays showing various chest conditions like pneumonia, nodules, or other pathologies\n",
    "\n",
    "The goal is to train a model that can automatically distinguish between normal and abnormal chest X-rays.\n",
    "\n",
    "1. Load the ChestMNIST dataset from MedMNIST using the provided code:\n",
    "   ```python\n",
    "   from medmnist import ChestMNIST\n",
    "   train_dataset = ChestMNIST(split='train', download=True)\n",
    "   test_dataset = ChestMNIST(split='test', download=True)\n",
    "   ```\n",
    "\n",
    "2. Create DataLoaders for both training and test datasets with appropriate batch sizes\n",
    "\n",
    "3. Visualize some examples from the dataset:\n",
    "   - Display a grid of sample images from both normal and abnormal classes (at least 3 examples from each class)\n",
    "   - Include the class labels in the plot titles\n",
    "   - Use matplotlib to create a clear visualization\n",
    "\n",
    "4. Modify the Net model to match ChestMNIST's number of classes (2 classes - normal vs abnormal)\n",
    "\n",
    "5. Train the model for 5 epochs and print the training loss after every 100 batches\n",
    "\n",
    "6. Evaluate the model's accuracy on the test set\n",
    "\n",
    "- **Hint #1:** The images are already in the correct size for our Net model (28x28), just normalize them\n",
    "- **Hint #2:** Use torch.cuda.is_available() to check if GPU is available and move data to device accordingly\n",
    "- **Hint #3:** The training loop should look similar to previous exercises, but make sure to handle the binary classification task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301a3b9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30d9f0ed",
   "metadata": {},
   "source": [
    "### ✏️**Exercise**\n",
    "\n",
    "In this exercise, you will train a simple CNN model (Net) on the DermaMNIST dataset. DermaMNIST is a large dataset of dermatoscopic images that can be used to detect various skin conditions.\n",
    "\n",
    "The dataset contains dermatoscopic images classified into 7 classes:\n",
    "- Class 0: Actinic keratoses and intraepithelial carcinoma\n",
    "- Class 1: Basal cell carcinoma \n",
    "- Class 2: Benign keratosis-like lesions\n",
    "- Class 3: Dermatofibroma\n",
    "- Class 4: Melanoma\n",
    "- Class 5: Melanocytic nevi\n",
    "- Class 6: Vascular lesions\n",
    "The goal is to train a model that can automatically classify different types of skin lesions.\n",
    "\n",
    "1. Load the DermaMNIST dataset from MedMNIST using the provided code:\n",
    "   ```python\n",
    "   from medmnist import DermaMNIST\n",
    "   train_dataset = DermaMNIST(split='train', download=True)\n",
    "   test_dataset = DermaMNIST(split='test', download=True)\n",
    "   ```\n",
    "\n",
    "2. Create DataLoaders for both training and test datasets with appropriate batch sizes\n",
    "\n",
    "3. Visualize some examples from the dataset:\n",
    "   - Display a grid of sample images showing examples from different classes\n",
    "   - Include the class labels in the plot titles\n",
    "   - Use matplotlib to create a clear visualization\n",
    "\n",
    "4. Modify the Net model to match DermaMNIST's number of classes (7 classes)\n",
    "\n",
    "5. Train the model for 5 epochs and print the training loss after every 100 batches\n",
    "\n",
    "6. Evaluate the model's accuracy on the test set\n",
    "\n",
    "- **Hint #1:** The images are already in the correct size for our Net model (28x28), just normalize them\n",
    "- **Hint #2:** Use torch.cuda.is_available() to check if GPU is available and move data to device accordingly\n",
    "- **Hint #3:** The training loop should look similar to previous exercises, but make sure to handle the multi-class classification task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1d5089",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
